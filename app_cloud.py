import os
import streamlit as st
# Disable OpenTelemetry instrumentation globally to prevent noisy telemetry
# events and related runtime errors.  This must be set before importing
# any Chroma or LangChain modules that might register OpenTelemetry hooks.
os.environ["OPENTELEMETRY_SDK_DISABLED"] = "true"
from chromadb.config import Settings
from typing import List, Dict, Any
from rag_cloud import chat_with_context_openai

# Use OpenAI embeddings via LangChain for cloud queries.  This must match the
# embedding model used during ingestion.
# Prefer the standalone langchain-ollama package for Ollama embeddings.  If
# langchain-ollama isn't installed, fall back to the (deprecated) community
# embedding class so that the app still works.  Users can install
# langchain-ollama via `pip install -U langchain-ollama`.
try:
    from langchain_ollama import OllamaEmbeddings  # type: ignore[import]
except ImportError:
    from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma  # type: ignore[import]

# Determine which embedding model to use.  Try Ollama first (if an Ollama
# service is running inside the container) and fall back to OpenAI.  The
# embedding model must match the one used in ingest.py; Ollama uses
# "nomic-embed-text" and OpenAI uses "text-embedding-3-small" by default.
def get_embeddings():
    try:
        ef = OllamaEmbeddings(model=os.environ.get("EMBED_MODEL", "nomic-embed-text"))
        # quick test call to ensure model availability
        _ = ef.embed_query("test")
        return ef
    except Exception:
        # fallback to OpenAI embeddings
        embed_model = os.environ.get("OPENAI_EMBED_MODEL", "text-embedding-3-small")
        return OpenAIEmbeddings(model=embed_model)

# Instantiate the embedding function once so it can be reused for all queries.
embedding_function = get_embeddings()


from pathlib import Path

# Use an absolute path for the DB relative to this file.  This ensures the
# vector store is discovered correctly regardless of the current working
# directory when the app is launched.
BASE_DIR = Path(__file__).resolve().parent
DB_DIR = str(BASE_DIR / "db")

COLLECTION_NAME = "som"  # must match ingest.py

st.set_page_config(page_title="Society of Man Rules Assistant", page_icon="🛰️", layout="wide")
st.title("🛰️ Society of Man — Rules Assistant (Railway)")

st.markdown("""
This cloud app uses a prebuilt index of your **Society of Man** rulebook.
If the answer isn't in your sources, it will say so directly.
""")

with st.sidebar:
    st.header("⚙️ Settings")
    top_k = st.slider("Results to retrieve", 1, 8, 4)
    temperature = st.slider("Response creativity", 0.0, 1.0, 0.2, 0.05)
    st.caption("Search uses your bundled Chroma DB; answers generated by OpenAI.")

# Open the persisted Chroma DB using LangChain.  Disable anonymized telemetry on
# the underlying chromadb client to avoid OpenTelemetry errors.
try:
    # Load the persistent Chroma DB using the same embedding function used at ingest time.
    vectordb = Chroma(
        persist_directory=DB_DIR,
        embedding_function=embedding_function,
        collection_name=COLLECTION_NAME,
        client_settings=Settings(anonymized_telemetry=False),
    )
except Exception:
    # If the database isn't found or can't be loaded, prompt the user to ingest their docs.
    st.error("No index found in the 'db' directory. Run ingest.py locally and commit the db/ folder.")
    st.stop()

def search(query: str, k: int = 4) -> List[Dict[str, Any]]:
    docs = vectordb.similarity_search(query, k)
    out = []
    for doc in docs:
        out.append({
            "text": doc.page_content,
            "source": doc.metadata.get("source", ""),
            "loc": doc.metadata.get("loc", ""),
        })
    return out

query = st.text_input('Ask a question (e.g., "How do Yellow Color features work?")')
go = st.button("Search")

if go and query.strip():
    with st.spinner("Consulting the Codex..."):
        chunks = search(query, k=top_k)
        if not chunks:
            st.info("No matching text found in the index.")
        else:
            answer = chat_with_context_openai(query, chunks, temperature=temperature)
            st.markdown("### 📘 Answer")
            st.write(answer)

            st.markdown("### 📑 Sources")
            for i, ch in enumerate(chunks, start=1):
                with st.expander(f"Chunk {i} — {ch['source']} {ch['loc']}"):
                    st.write(ch["text"])