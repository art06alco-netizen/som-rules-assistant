import os
import streamlit as st
# Disable OpenTelemetry instrumentation globally to prevent noisy telemetry
# events and related runtime errors.  This must be set before importing
# any Chroma or LangChain modules that might register OpenTelemetry hooks.
os.environ["OPENTELEMETRY_SDK_DISABLED"] = "true"

# Ensure an OpenAI API key is available for embedding and chat.  This default
# key will be used if OPENAI_API_KEY is not already defined in the
# environment.  In a real deployment you should set the API key via
# environment variables rather than hard-coding it in source code.
os.environ.setdefault("OPENAI_API_KEY", "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")
from chromadb.config import Settings
from typing import List, Dict, Any
from rag_cloud import chat_with_context_openai

# Use OpenAI embeddings via LangChain for cloud queries.  This must match the
# embedding model used during ingestion.
# Import only the OpenAI embeddings.  We rely solely on OpenAI for
# embedding generation in the cloud to avoid any dependence on a
# local Ollama server.  Override the model via the OPENAI_EMBED_MODEL
# environment variable if needed.
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma  # type: ignore[import]

# Determine which embedding model to use.  We always select an OpenAI
# embedding model in the cloud environment.  The model must match
# the one used during ingestion; by default we use
# "text-embedding-3-small".
def get_embeddings() -> OpenAIEmbeddings:
    """
    Return an OpenAI embeddings instance for the cloud app.  This ensures
    that both ingestion and retrieval use the same embedding model and
    avoids any dependence on Ollama.  The embedding model can be
    overridden via the OPENAI_EMBED_MODEL environment variable.
    """
    embed_model = os.environ.get("OPENAI_EMBED_MODEL", "text-embedding-3-small")
    return OpenAIEmbeddings(model=embed_model)

# Instantiate the embedding function once so it can be reused for all queries.
embedding_function = get_embeddings()


from pathlib import Path

# Use an absolute path for the DB relative to this file.  This ensures the
# vector store is discovered correctly regardless of the current working
# directory when the app is launched.
BASE_DIR = Path(__file__).resolve().parent
DB_DIR = str(BASE_DIR / "db")

COLLECTION_NAME = "som"  # must match ingest.py

st.set_page_config(page_title="Society of Man Rules Assistant", page_icon="🛰️", layout="wide")
st.title("🛰️ Society of Man — Rules Assistant (Railway)")

st.markdown("""
This cloud app uses a prebuilt index of your **Society of Man** rulebook.
If the answer isn't in your sources, it will say so directly.
""")

with st.sidebar:
    st.header("⚙️ Settings")
    top_k = st.slider("Results to retrieve", 1, 8, 4)
    temperature = st.slider("Response creativity", 0.0, 1.0, 0.2, 0.05)
    st.caption("Search uses your bundled Chroma DB; answers generated by OpenAI.")

# Open the persisted Chroma DB using LangChain.  Disable anonymized telemetry on
# the underlying chromadb client to avoid OpenTelemetry errors.
try:
    # Load the persistent Chroma DB using the same embedding function used at ingest time.
    vectordb = Chroma(
        persist_directory=DB_DIR,
        embedding_function=embedding_function,
        collection_name=COLLECTION_NAME,
        client_settings=Settings(anonymized_telemetry=False),
    )
except Exception:
    # If the database isn't found or can't be loaded, prompt the user to ingest their docs.
    st.error("No index found in the 'db' directory. Run ingest.py locally and commit the db/ folder.")
    st.stop()

def search(query: str, k: int = 4) -> List[Dict[str, Any]]:
    docs = vectordb.similarity_search(query, k)
    out = []
    for doc in docs:
        out.append({
            "text": doc.page_content,
            "source": doc.metadata.get("source", ""),
            "loc": doc.metadata.get("loc", ""),
        })
    return out

query = st.text_input('Ask a question (e.g., "How do Yellow Color features work?")')
go = st.button("Search")

if go and query.strip():
    with st.spinner("Consulting the Codex..."):
        chunks = search(query, k=top_k)
        if not chunks:
            st.info("No matching text found in the index.")
        else:
            answer = chat_with_context_openai(query, chunks, temperature=temperature)
            st.markdown("### 📘 Answer")
            st.write(answer)

            st.markdown("### 📑 Sources")
            for i, ch in enumerate(chunks, start=1):
                with st.expander(f"Chunk {i} — {ch['source']} {ch['loc']}"):
                    st.write(ch["text"])